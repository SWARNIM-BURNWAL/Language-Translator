{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention in Transformers\n",
    "\n",
    "## There are three parameters:-\n",
    "\n",
    "- Q (Query Vector) :- What i am looking for? (d_q or dimension_of_query )\n",
    "- K (Key Vector) :- What i can offer ? (d_k or dimension_of_keys)\n",
    "- V (Value Vector) :- What i am offering ? (d_v or dimension_of_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q (Query vector):\n",
      " [[-0.84255448  1.69118479  0.16840026 -0.33852743 -0.86381845  0.87873148\n",
      "  -0.43893728  1.43357632]\n",
      " [-0.36916007  0.99008703  1.26395118  1.25426793 -1.14313892  0.44420488\n",
      "  -1.79295722  0.08786969]\n",
      " [-0.93264763  2.10190512 -2.28282036 -0.28115302  0.16496563 -3.86691182\n",
      "  -0.36078562 -0.33304909]\n",
      " [ 0.70902575  0.90507036 -0.612227    0.29584081 -1.27537195 -0.53748187\n",
      "   1.310467   -1.05024303]] \n",
      "\n",
      "K (Key vector):\n",
      " [[-0.13831726  1.61754182 -0.38570355 -0.06724541 -0.32901251  0.43812086\n",
      "   0.83568471 -0.88561807]\n",
      " [ 0.44164728 -1.12268517 -0.8300944  -0.34109228  0.3553728  -0.61233613\n",
      "  -1.15187035  0.1823838 ]\n",
      " [-0.55582771  0.5160848  -0.31469653  2.05612718 -1.4826562  -2.49664229\n",
      "   1.7230993   1.99282134]\n",
      " [-0.05107385 -1.41396277  0.06189906 -0.12581689  1.04094493 -0.03749632\n",
      "   0.53257072  0.88856998]] \n",
      "\n",
      "V (Value vector):\n",
      " [[ 0.30484211  1.62034263  0.36740884 -0.98962018  0.09368226 -1.75405572\n",
      "   0.03991657 -0.06351192]\n",
      " [-0.4504077  -0.10019523 -0.97578105  0.76889551 -0.87555141 -0.47108151\n",
      "   0.580257    0.06575303]\n",
      " [-0.65572362  0.30078728 -0.61406206 -1.07976532  1.16208255  0.20395302\n",
      "  -0.33884634  1.81876984]\n",
      " [ 0.79778368  1.25778452  0.05043346  0.27569132  0.53397663  1.18342807\n",
      "   0.12113994 -0.43886321]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from typing import List\n",
    "\n",
    "\n",
    "sequence_length, dimension_of_keys, dimension_of_values = 4, 8, 8\n",
    "# np.random.seed(seed=43)\n",
    "query = np.random.randn(sequence_length, dimension_of_keys)\n",
    "key = np.random.randn(sequence_length, dimension_of_keys)\n",
    "value = np.random.randn(sequence_length, dimension_of_values)\n",
    "\n",
    "print(f\"Q (Query vector):\\n {query} \\n\")\n",
    "print(f\"K (Key vector):\\n {key} \\n\")\n",
    "print(f\"V (Value vector):\\n {value} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention\n",
    "\n",
    "### ![\"\"](https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.84269718, -2.37309569,  1.77945603, -2.18729245],\n",
       "       [ 0.07527066, -1.34857736,  0.56885814, -3.54406134],\n",
       "       [ 2.67331963,  2.00048616,  9.86779345, -3.20167848],\n",
       "       [ 3.7915456 , -2.12082091,  4.27190508, -2.93380296]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ! Solving inside the softmax\n",
    "np.matmul(query, key.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.5125745535077024, 1.0357473030624733, 11.93390494969697)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# But why are we diving the equation by the square root of dimension of the keys ?\n",
    "query.var(), key.var(), np.matmul(query, key.T).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.5125745535077024, 1.0357473030624733, 1.4917381187121208)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just to reduce the variance and to stabilize the values of query and key vector\n",
    "scaled = np.matmul(query, key.T)/math.sqrt(dimension_of_keys)\n",
    "query.var(), key.var(), scaled.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.65149184, -0.83901603,  0.62913271, -0.77332466],\n",
       "       [ 0.0266122 , -0.4767941 ,  0.20112173, -1.2530149 ],\n",
       "       [ 0.94516122,  0.70727867,  3.48879183, -1.13196428],\n",
       "       [ 1.3405138 , -0.74982342,  1.51034652, -1.03725599]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking\n",
    "\n",
    "- Used to prevent to get the context from the words that will be generated in future\n",
    "- no required encoder cause we are providing the input simultaneously\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masking = np.tril(np.ones((sequence_length, sequence_length)))\n",
    "masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masking[masking==0]= \"-inf\"\n",
    "# masking[masking==1]= 0\n",
    "# masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masking = np.where(masking == 0, -np.inf, 0)\n",
    "masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.65149184,        -inf,        -inf,        -inf],\n",
       "       [ 0.0266122 , -0.4767941 ,        -inf,        -inf],\n",
       "       [ 0.94516122,  0.70727867,  3.48879183,        -inf],\n",
       "       [ 1.3405138 , -0.74982342,  1.51034652, -1.03725599]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled+masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "$$\n",
    "\\text{softmax} = \\frac{e^{x_i}}{\\sum_j e^x_j}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x: List[float]) -> List[float]:\n",
    "    return (np.exp(x).T/np.sum(np.exp(x), axis=-1)).T\n",
    "# lambda x:(np.exp(x).T/np.sum(np.exp(x),axis=1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.62325949, 0.37674051, 0.        , 0.        ],\n",
       "       [0.06889859, 0.05431243, 0.87678898, 0.        ],\n",
       "       [0.41640481, 0.05148655, 0.49348409, 0.03862455]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying Attention\n",
    "attention = softmax(scaled+masking)\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but if i remove the masking\n",
    "# attention=softmax(scaled)\n",
    "# attention\n",
    "\n",
    "# here all the values in the row are summing upto 1 because this a praobability distribution ,which we dont want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.30484211,  1.62034263,  0.36740884, -0.98962018,  0.09368226,\n",
       "        -1.75405572,  0.03991657, -0.06351192],\n",
       "       [ 0.02030891,  0.97214632, -0.1386252 , -0.32711608, -0.27146733,\n",
       "        -1.27070736,  0.2434847 , -0.01481258],\n",
       "       [-0.57839079,  0.36992445, -0.56608594, -0.97314918,  0.97780242,\n",
       "         0.03238621, -0.26283137,  1.59387267],\n",
       "       [-0.18902735,  0.86657486, -0.19833068, -0.89469337,  0.58802447,\n",
       "        -0.60829466, -0.11603942,  0.85752181]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So this is encapsulating more of context of the words\n",
    "new_values = np.matmul(attention, value)\n",
    "new_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.30484211,  1.62034263,  0.36740884, -0.98962018,  0.09368226,\n",
       "        -1.75405572,  0.03991657, -0.06351192],\n",
       "       [-0.4504077 , -0.10019523, -0.97578105,  0.76889551, -0.87555141,\n",
       "        -0.47108151,  0.580257  ,  0.06575303],\n",
       "       [-0.65572362,  0.30078728, -0.61406206, -1.07976532,  1.16208255,\n",
       "         0.20395302, -0.33884634,  1.81876984],\n",
       "       [ 0.79778368,  1.25778452,  0.05043346,  0.27569132,  0.53397663,\n",
       "         1.18342807,  0.12113994, -0.43886321]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binding Evrything into a function\n",
    "\n",
    "![\"\"](https://production-media.paperswithcode.com/methods/35184258-10f5-4cd0-8de3-bd9bc8f88dc3.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Self_Attention:\n",
    "    def __init__(self, length: List[float], q: List[float], k: List[float], v: List[float], mask: List[float]) -> None:\n",
    "        self.query = q\n",
    "        self.key = k\n",
    "        self.value = v\n",
    "        self.mask = mask\n",
    "        self.length = length\n",
    "\n",
    "    def softmax(self, x: List[float]) -> List[float]:\n",
    "        return (np.exp(x).T/np.sum(np.exp(x), axis=-1)).T\n",
    "\n",
    "    def scaled_dot_product_attention(self):\n",
    "        dimension_of_keys = self.query.shape[-1]\n",
    "        scaled = np.matmul(self.query, self.key.T)/math.sqrt(dimension_of_keys)\n",
    "        if self.mask is not None:\n",
    "            scaled = scaled+self.mask\n",
    "        attention = softmax(scaled)\n",
    "        output = np.matmul(attention, self.value)\n",
    "\n",
    "        return attention, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attention Scores:\n",
      "\n",
      "[[1.         0.         0.         0.        ]\n",
      " [0.62325949 0.37674051 0.         0.        ]\n",
      " [0.06889859 0.05431243 0.87678898 0.        ]\n",
      " [0.41640481 0.05148655 0.49348409 0.03862455]]\n",
      "\n",
      "Output:\n",
      "\n",
      "[[ 0.30484211  1.62034263  0.36740884 -0.98962018  0.09368226 -1.75405572\n",
      "   0.03991657 -0.06351192]\n",
      " [ 0.02030891  0.97214632 -0.1386252  -0.32711608 -0.27146733 -1.27070736\n",
      "   0.2434847  -0.01481258]\n",
      " [-0.57839079  0.36992445 -0.56608594 -0.97314918  0.97780242  0.03238621\n",
      "  -0.26283137  1.59387267]\n",
      " [-0.18902735  0.86657486 -0.19833068 -0.89469337  0.58802447 -0.60829466\n",
      "  -0.11603942  0.85752181]]\n"
     ]
    }
   ],
   "source": [
    "test = Self_Attention(length=sequence_length, q=query,\n",
    "                      v=value, k=key, mask=masking)\n",
    "attention, output = test.scaled_dot_product_attention()\n",
    "print(\"\\nAttention Scores:\\n\")\n",
    "print(attention)\n",
    "print(\"\\nOutput:\\n\")\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
