{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why are we using Layer Normalisation ?\n",
    "\n",
    "- During trainning a transformer model due to backpropagation , a point comes where the gradients starts to disapper and eventually the model stops learning , so inorder to prevent this we use Add & Norm layer which takes input from multi-headed attention but also it has residual connections from the output matrix just after applying positional encoding.\n",
    "  > Layer Normalisation , What and why ...?\n",
    "\n",
    "> What is Normalisation?\n",
    "\n",
    "- After applying activation function the values we get are either extreme positive or extreme negative by applying `Normalisation` we encapsulate the values into a much smaller range , typically centering it around zero `0`\n",
    "-\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.Tensor([\n",
    "    [\n",
    "        [0.2, 0.1, 0.3],\n",
    "        [0.5, 0.1, 0.1]\n",
    "    ]\n",
    "]) # here we have added a batch dimension to the input just to make training faster\n",
    "\n",
    "B,S,E=inputs.size()\n",
    "# B=Batch size, S=Sequence length, E=Embedding size\n",
    "inputs= inputs.reshape(S,B,E)\n",
    "inputs.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3]), torch.Size([1, 3]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameter_shape=inputs.size()[-2:]\n",
    "print(len(parameter_shape))\n",
    "Gamma=nn.Parameter(torch.ones(parameter_shape))\n",
    "Beta=nn.Parameter(torch.zeros(parameter_shape))\n",
    "Gamma.size(),Beta.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, -2]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimension=[-(i+1) for i in range(len(parameter_shape))] # finding the dimension across which the layer norm will be applied Batch Dimension & Embedding Dimension\n",
    "dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 1])\n",
      "tensor([[[0.2000]],\n",
      "\n",
      "        [[0.2333]]])\n"
     ]
    }
   ],
   "source": [
    "## calculating mean\n",
    "mean=inputs.mean(dimension,keepdim=True)\n",
    "print(mean.size())\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 1]),\n",
       " tensor([[[0.0817]],\n",
       " \n",
       "         [[0.1886]]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variance=(inputs-mean).pow(2).mean(dimension,keepdim=True)\n",
    "epilson=1e-5 # to ensure that the denominator is never zero\n",
    "std=torch.sqrt(variance+epilson)\n",
    "std.size(),std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 3]),\n",
       " tensor([[[ 0.0000, -1.2238,  1.2238]],\n",
       " \n",
       "         [[ 1.4140, -0.7070, -0.7070]]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=(inputs-mean)/std # this is the layer norm output\n",
    "output=Gamma*y+Beta\n",
    "output.size(),output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: \n",
      " tensor([[[0.2000]],\n",
      "\n",
      "        [[0.2333]]]) \n",
      " Mean Size:  torch.Size([2, 1, 1])\n",
      "Variance: \n",
      " tensor([[[0.0067]],\n",
      "\n",
      "        [[0.0356]]]) \n",
      " Variance Size:  torch.Size([2, 1, 1])\n",
      "Standard Deviation: \n",
      " tensor([[[0.0817]],\n",
      "\n",
      "        [[0.1886]]]) \n",
      " Standard Deviation Size:  torch.Size([2, 1, 1])\n",
      "Y:  tensor([[[ 0.0000, -1.2238,  1.2238]],\n",
      "\n",
      "        [[ 1.4140, -0.7070, -0.7070]]]) \n",
      " Y Size:  torch.Size([2, 1, 3])\n",
      "Output:  tensor([[[ 0.0000, -1.2238,  1.2238]],\n",
      "\n",
      "        [[ 1.4140, -0.7070, -0.7070]]], grad_fn=<AddBackward0>) \n",
      " Output Size:  torch.Size([2, 1, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 3]),\n",
       " tensor([[[ 0.0000, -1.2238,  1.2238]],\n",
       " \n",
       "         [[ 1.4140, -0.7070, -0.7070]]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNormalisation():\n",
    "    def __init__(self,parameter_shape,epsilon=1e-5)->None:\n",
    "        self.Gamma=nn.Parameter(torch.ones(parameter_shape))\n",
    "        self.Beta=nn.Parameter(torch.zeros(parameter_shape))\n",
    "        self.parameter_shape=parameter_shape\n",
    "        self.epsilon=epsilon\n",
    "\n",
    "    def forward(self,inputs:torch.Tensor)->torch.Tensor:\n",
    "        dimension=[-(i+1) for i in range(len(self.parameter_shape))]\n",
    "        mean=inputs.mean(dimension,keepdim=True)\n",
    "        print(f\"Mean: \\n {mean} \\n Mean Size:  {mean.size()}\")\n",
    "        variance=(inputs-mean).pow(2).mean(dimension,keepdim=True)\n",
    "        print(f\"Variance: \\n {variance} \\n Variance Size:  {variance.size()}\")\n",
    "        std=torch.sqrt(variance+self.epsilon)\n",
    "        print(f\"Standard Deviation: \\n {std} \\n Standard Deviation Size:  {std.size()}\")\n",
    "        y=(inputs-mean)/std\n",
    "        print(f\"Y:  {y} \\n Y Size:  {y.size()}\")\n",
    "        output=self.Gamma*y+self.Beta\n",
    "        print(f\"Output:  {output} \\n Output Size:  {output.size()}\")\n",
    "        return output\n",
    "\n",
    "\n",
    "layer_norm=LayerNormalisation(parameter_shape)\n",
    "output=layer_norm.forward(inputs)\n",
    "output.size(),output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
